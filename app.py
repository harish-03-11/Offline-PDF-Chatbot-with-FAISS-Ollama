# app.py
import pickle
import numpy as np
import subprocess
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import faiss

app = FastAPI()

# Load the FAISS index and metadata (generated by process_pdf.py)
INDEX_FILE = "embeddings/faiss_index.pkl"
with open(INDEX_FILE, "rb") as f:
    data = pickle.load(f)
    index = data["index"]
    chunks = data["chunks"]

# Load the same embedding model used in process_pdf.py
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Define the request model for incoming queries
class QueryRequest(BaseModel):
    query: str

def generate_response(context: str, query: str) -> str:
    # Truncate the context to a maximum of 200 characters to keep the prompt short
    truncated_context = context[:200]
    prompt = f"Context:\n{truncated_context}\n\nUser Query: {query}\n\nAnswer:"
    command = [
        "ollama",
        "run",
        "llama2",   # Replace with your Ollama model name if different
        prompt      # Pass the prompt as a positional argument
    ]
    try:
        # Use a 60-second timeout for this command
        result = subprocess.run(command, capture_output=True, text=True, timeout=60)
        if result.returncode != 0:
            return "Ollama error: " + result.stderr.strip()
        return result.stdout.strip()
    except subprocess.TimeoutExpired:
        return "Ollama error: Command timed out after 60 seconds."
    except Exception as e:
        return f"Error calling Ollama: {e}"

@app.post("/query")
async def query_pdf(request: QueryRequest):
    # Embed the user query
    query_embedding = embedding_model.encode([request.query]).astype("float32")
    
    # Retrieve only the top matching chunk (reduce k to 1)
    k = 1
    distances, indices = index.search(query_embedding, k)
    
    retrieved_chunks = []
    for idx in indices[0]:
        if idx >= 0 and idx < len(chunks):
            retrieved_chunks.append(chunks[idx])
    
    # Combine retrieved chunks (if more than one, join them with a separator)
    context = "\n---\n".join(retrieved_chunks)
    
    # Generate and return the response using Ollama
    response = generate_response(context, request.query)
    return {"response": response}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
